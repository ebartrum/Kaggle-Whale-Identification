{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_augmentation import random_transform\n",
    "\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Reshape, Flatten, Input, merge, subtract\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resize_shape = (128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For now, we remove new_whale\n",
    "# data = data[data['Id'] != 'new_whale'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9850"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00022e1a.jpg</td>\n",
       "      <td>w_e15442c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000466c4.jpg</td>\n",
       "      <td>w_1287fbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00087b01.jpg</td>\n",
       "      <td>w_da2efe0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001296d5.jpg</td>\n",
       "      <td>w_19e5482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014cfdf.jpg</td>\n",
       "      <td>w_f22f3e3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0025e8c2.jpg</td>\n",
       "      <td>w_8b1ca89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0026a8ab.jpg</td>\n",
       "      <td>w_eaad6a8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0031c258.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0035632e.jpg</td>\n",
       "      <td>w_3d0bc7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0037e7d3.jpg</td>\n",
       "      <td>w_50db782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image         Id\n",
       "0  00022e1a.jpg  w_e15442c\n",
       "1  000466c4.jpg  w_1287fbc\n",
       "2  00087b01.jpg  w_da2efe0\n",
       "3  001296d5.jpg  w_19e5482\n",
       "4  0014cfdf.jpg  w_f22f3e3\n",
       "5  0025e8c2.jpg  w_8b1ca89\n",
       "6  0026a8ab.jpg  w_eaad6a8\n",
       "7  0031c258.jpg  new_whale\n",
       "8  0035632e.jpg  w_3d0bc7a\n",
       "9  0037e7d3.jpg  w_50db782"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('data/train/00022e1a.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 699)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 699)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_image = np.stack([image]*3,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 699, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(new_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gray = np.mean(image, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(random_transform(gray),cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# All images (if small) can be held in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = data['Image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image(file, shape=(resize_shape[0],resize_shape[1])):\n",
    "    image = Image.open('data/train/' + file)\n",
    "    image = image.resize(shape)\n",
    "    image = np.array(image)\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.stack([image]*3,axis=2) \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_list = [get_image(f) for f in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_list = [len(image.shape) for image in image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00022e1a.jpg'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['image_array'] = image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "      <th>image_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00022e1a.jpg</td>\n",
       "      <td>w_e15442c</td>\n",
       "      <td>[[[196, 196, 196], [191, 191, 191], [195, 195,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000466c4.jpg</td>\n",
       "      <td>w_1287fbc</td>\n",
       "      <td>[[[196, 189, 183], [194, 187, 181], [198, 191,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00087b01.jpg</td>\n",
       "      <td>w_da2efe0</td>\n",
       "      <td>[[[191, 191, 191], [192, 192, 192], [186, 186,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001296d5.jpg</td>\n",
       "      <td>w_19e5482</td>\n",
       "      <td>[[[119, 140, 219], [121, 142, 221], [122, 143,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image         Id                                        image_array\n",
       "0  00022e1a.jpg  w_e15442c  [[[196, 196, 196], [191, 191, 191], [195, 195,...\n",
       "1  000466c4.jpg  w_1287fbc  [[[196, 189, 183], [194, 187, 181], [198, 191,...\n",
       "2  00087b01.jpg  w_da2efe0  [[[191, 191, 191], [192, 192, 192], [186, 186,...\n",
       "3  001296d5.jpg  w_19e5482  [[[119, 140, 219], [121, 142, 221], [122, 143,..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "\n",
    "test_proportion = 0.8\n",
    "cutoff_index = int(len(data) * test_proportion)\n",
    "\n",
    "training_data = data.iloc[:cutoff_index].reset_index(drop=True)\n",
    "test_data = data.iloc[cutoff_index:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "training_counts = Counter(training_data['Id'])\n",
    "training_data['Id_count'] = training_data.apply(lambda x: training_counts.get(x[\"Id\"]), axis=1)\n",
    "\n",
    "test_counts = Counter(test_data['Id'])\n",
    "test_data['Id_count'] = test_data.apply(lambda x: test_counts.get(x[\"Id\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_triple(data):\n",
    "#     filtered = data[data['Id_count'] > 1].reset_index(drop=True)\n",
    "    filtered = data[(data.Id_count > 1) & (data.Id != 'new_whale')].reset_index(drop=True)\n",
    "    \n",
    "    anchor_index = randint(0,len(filtered)-1)\n",
    "    anchor_image = filtered['image_array'][anchor_index]\n",
    "    anchor_id = filtered['Id'][anchor_index]   \n",
    "    same_id = anchor_id\n",
    "    relevant_indices = list(filtered.index[filtered['Id'] == anchor_id])\n",
    "    same_index = np.random.choice(relevant_indices)\n",
    "    same_image = filtered['image_array'][same_index]\n",
    "    \n",
    "    different_id = anchor_id\n",
    "    while (anchor_id == different_id):\n",
    "        different_index = randint(0,len(data)-1)\n",
    "        different_id = data['Id'][different_index]\n",
    "    \n",
    "    different_image = data['image_array'][different_index]\n",
    "    anchor_image = random_transform(anchor_image)\n",
    "    same_image = random_transform(same_image)\n",
    "    different_image = random_transform(different_image)\n",
    "    return anchor_image, same_image, different_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triple_generator(batch_size, data, resize_shape):\n",
    "    while True:\n",
    "        anchor_batch = np.zeros((batch_size, resize_shape[0],resize_shape[1],resize_shape[2]))\n",
    "        same_image_batch = np.zeros((batch_size, resize_shape[0],resize_shape[1],resize_shape[2]))\n",
    "        different_image_batch = np.zeros((batch_size, resize_shape[0],resize_shape[1],resize_shape[2]))\n",
    "        for i in range(batch_size):\n",
    "                anchor_batch[i,:,:,:], same_image_batch[i,:,:,:], different_image_batch[i,:,:,:] = get_triple(data)\n",
    "\n",
    "        batches = [anchor_batch, same_image_batch, different_image_batch]\n",
    "        yield batches, np.ones(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def L2_distance(X):\n",
    "\n",
    "#     encoded_l, encoded_r = X\n",
    "\n",
    "#     # BPR loss\n",
    "#     loss = 1.0 - K.sigmoid(\n",
    "#         K.sum(user_latent * positive_item_latent, axis=-1, keepdims=True) -\n",
    "#         K.sum(user_latent * negative_item_latent, axis=-1, keepdims=True))\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = resize_shape\n",
    "\n",
    "anchor_input = Input(input_shape)\n",
    "same_category_input = Input(input_shape)\n",
    "different_category_input = Input(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"lo...)`\n"
     ]
    }
   ],
   "source": [
    "def bpr_triplet_loss(X):    \n",
    "    alpha = 3\n",
    "\n",
    "    anchor_embedding, same_embedding, different_embedding = X\n",
    "    \n",
    "    positive_distance = K.square(anchor_embedding - same_embedding)\n",
    "    negative_distance = K.square(anchor_embedding - different_embedding)\n",
    "    \n",
    "    positive_distance = K.mean(positive_distance, axis=-1, keepdims=True)\n",
    "    negative_distance = K.mean(negative_distance, axis=-1, keepdims=True)\n",
    "    \n",
    "    loss = K.maximum(0.0, alpha + positive_distance - negative_distance)\n",
    "   \n",
    "    return K.mean(loss)\n",
    "\n",
    "#Something like this:\n",
    "# def triplet_loss(inputs, dist='sqeuclidean', margin='maxplus'):\n",
    "#     anchor, positive, negative = inputs\n",
    "#     positive_distance = K.square(anchor - positive)\n",
    "#     negative_distance = K.square(anchor - negative)\n",
    "#     if dist == 'euclidean':\n",
    "#         positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True))\n",
    "#         negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n",
    "#     elif dist == 'sqeuclidean':\n",
    "#         positive_distance = K.mean(positive_distance, axis=-1, keepdims=True)\n",
    "#         negative_distance = K.mean(negative_distance, axis=-1, keepdims=True)\n",
    "#     loss = positive_distance - negative_distance\n",
    "#     if margin == 'maxplus':\n",
    "#         loss = K.maximum(0.0, 1 + loss)\n",
    "#     elif margin == 'softplus':\n",
    "#         loss = K.log(1 + K.exp(loss))\n",
    "#     return K.mean(loss)\n",
    "\n",
    "\n",
    "\n",
    "convnet = Sequential()\n",
    "convnet.add(Conv2D(filters=8, input_shape=(\n",
    "    resize_shape[0],resize_shape[1],resize_shape[2],), kernel_size=5, activation='relu',name='conv_1'))\n",
    "convnet.add(MaxPooling2D(pool_size=2,name='pool_1'))\n",
    "convnet.add(Conv2D(filters=12, kernel_size=3, activation='relu',name='conv_2'))\n",
    "convnet.add(MaxPooling2D(pool_size=2,name='pool_2'))\n",
    "convnet.add(Conv2D(filters=16, kernel_size=3, activation='relu',name='conv_3'))\n",
    "convnet.add(MaxPooling2D(pool_size=2,name='pool_3'))\n",
    "convnet.add(Conv2D(filters=20, kernel_size=3, activation='relu',name='conv_4'))\n",
    "convnet.add(Conv2D(filters=32, kernel_size=3, activation='relu',name='conv_5'))\n",
    "convnet.add(MaxPooling2D(pool_size=2,name='pool_4'))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(units=128, activation='relu',name='dense_1'))\n",
    "convnet.add(Dense(units=128, activation='relu',name='dense_2'))\n",
    "convnet.add(Dense(units=64, activation='relu',name='dense_3'))\n",
    "\n",
    "anchor = convnet(anchor_input)\n",
    "same = convnet(same_category_input)\n",
    "different = convnet(different_category_input)\n",
    "\n",
    "loss = merge(\n",
    "        [anchor, same, different],\n",
    "        mode=bpr_triplet_loss,\n",
    "        name='loss',\n",
    "        output_shape=(1, ))\n",
    "    \n",
    "siamese_net = Model(input=[anchor_input,same_category_input, different_category_input],output=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0000000000001\n",
    "# siamese_net.compile(loss=identity_loss,optimizer=SGD(LEARNING_RATE))\n",
    "siamese_net.compile(loss=identity_loss,optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "training_data_generator = triple_generator(BATCH_SIZE, training_data, resize_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 65s - loss: 0.4289    \n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 44s - loss: 0.4041    \n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 15s - loss: 0.4462    \n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 43s - loss: 0.4328    \n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 44s - loss: 0.4228    \n",
      "Epoch 6/100\n",
      " 3/10 [========>.....................] - ETA: 11s - loss: 0.4934"
     ]
    }
   ],
   "source": [
    "history = siamese_net.fit_generator(training_data_generator,\n",
    "                                    verbose=1, \n",
    "                                    epochs=100, \n",
    "                                    steps_per_epoch=10,\n",
    "                                    workers=16,\n",
    "                                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title(\"Loss function\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data_generator = triple_generator(BATCH_SIZE, test_data, resize_shape)\n",
    "evaluation_steps = 20\n",
    "metric_names = siamese_net.metrics_names\n",
    "metric_values = siamese_net.evaluate_generator(evaluation_data_generator, steps=evaluation_steps)\n",
    "print(\"Metric names\", metric_names)\n",
    "print(\"Metric values\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "now = time.strftime('%Y.%m.%d %H:%M:%S')\n",
    "directory = \"weights/\" + now + \"/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "\n",
    "siamese_net.save_weights(directory + \"siamese_weights\")\n",
    "convnet.save_weights(directory + \"convnet_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "file_list = data['Image']\n",
    "image_list = [get_image(f) for f in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_list = data['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = convnet.predict(np.stack(image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_image(file, shape=(resize_shape[0],resize_shape[1])):\n",
    "    image = Image.open('data/test/' + file)\n",
    "    image = image.resize(shape)\n",
    "    image = np.array(image)\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.stack([image]*3,axis=2) \n",
    "    return image\n",
    "\n",
    "\n",
    "sample_sub = pd.read_csv(\"data/sample_submission.csv\")\n",
    "submission_file_list = sample_sub['Image']\n",
    "submission_image_list = [get_sub_image(f) for f in submission_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_embedding_list = convnet.predict(np.stack(submission_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1,2,3,4]\n",
    "Y = [9,8,2,3]\n",
    "\n",
    "[x for (y,x) in sorted(zip(Y,X), key=lambda pair: pair[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(li):\n",
    "    my_set = set()\n",
    "    filtered = []\n",
    "    for e in li:\n",
    "        if e not in my_set:\n",
    "            filtered.append(e)\n",
    "            my_set.add(e)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def classify(image_embedding, embedding_list, id_list, num_categories=5):\n",
    "    image_embedding = np.expand_dims(image_embedding, axis=0)\n",
    "    stacked_image = np.repeat(image_embedding,len(embedding_list),axis=0)\n",
    "    square_differences = (stacked_image - embedding_list)**2\n",
    "    scores = np.sum(square_differences, axis=1)    \n",
    "    sorted_ids = [x for (y,x) in sorted(\n",
    "        zip(scores,id_list), key=lambda pair: pair[0])]\n",
    "    \n",
    "    return ' '.join(remove_duplicates(sorted_ids[0:num_categories]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(embedding_list[7], embedding_list, id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_prediction_list = [classify(image_embedding, embedding_list, id_list) for image_embedding in submission_embedding_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'Image': submission_file_list, 'Id': submission_prediction_list}, columns=['Image','Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"results/submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
