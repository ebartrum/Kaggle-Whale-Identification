{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_augmentation import random_transform\n",
    "\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import applications\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Reshape, Flatten, Input, merge, subtract, Lambda, Dropout\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resize_shape = (256, 256, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For now, we remove new_whale\n",
    "# data = data[data['Id'] != 'new_whale'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9850"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00022e1a.jpg</td>\n",
       "      <td>w_e15442c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000466c4.jpg</td>\n",
       "      <td>w_1287fbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00087b01.jpg</td>\n",
       "      <td>w_da2efe0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001296d5.jpg</td>\n",
       "      <td>w_19e5482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014cfdf.jpg</td>\n",
       "      <td>w_f22f3e3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0025e8c2.jpg</td>\n",
       "      <td>w_8b1ca89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0026a8ab.jpg</td>\n",
       "      <td>w_eaad6a8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0031c258.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0035632e.jpg</td>\n",
       "      <td>w_3d0bc7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0037e7d3.jpg</td>\n",
       "      <td>w_50db782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image         Id\n",
       "0  00022e1a.jpg  w_e15442c\n",
       "1  000466c4.jpg  w_1287fbc\n",
       "2  00087b01.jpg  w_da2efe0\n",
       "3  001296d5.jpg  w_19e5482\n",
       "4  0014cfdf.jpg  w_f22f3e3\n",
       "5  0025e8c2.jpg  w_8b1ca89\n",
       "6  0026a8ab.jpg  w_eaad6a8\n",
       "7  0031c258.jpg  new_whale\n",
       "8  0035632e.jpg  w_3d0bc7a\n",
       "9  0037e7d3.jpg  w_50db782"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('data/train/00022e1a.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 699)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 699)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_image = np.stack([image]*3,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 699, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(new_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gray = np.mean(image, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(random_transform(gray),cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# All images (if small) can be held in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = data['Image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image(file, shape=(resize_shape[0],resize_shape[1])):\n",
    "    image = Image.open('data/train/' + file)\n",
    "    image = image.resize(shape)\n",
    "    image = np.array(image)\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.stack([image]*3,axis=2) \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_list = [get_image(f) for f in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length_list = [len(image.shape) for image in image_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00022e1a.jpg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['image_array'] = image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "      <th>image_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00022e1a.jpg</td>\n",
       "      <td>w_e15442c</td>\n",
       "      <td>[[[240, 240, 240], [229, 229, 229], [230, 230,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000466c4.jpg</td>\n",
       "      <td>w_1287fbc</td>\n",
       "      <td>[[[198, 191, 185], [198, 191, 185], [194, 187,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00087b01.jpg</td>\n",
       "      <td>w_da2efe0</td>\n",
       "      <td>[[[180, 180, 180], [178, 178, 178], [183, 183,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001296d5.jpg</td>\n",
       "      <td>w_19e5482</td>\n",
       "      <td>[[[118, 139, 218], [119, 140, 219], [120, 141,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image         Id                                        image_array\n",
       "0  00022e1a.jpg  w_e15442c  [[[240, 240, 240], [229, 229, 229], [230, 230,...\n",
       "1  000466c4.jpg  w_1287fbc  [[[198, 191, 185], [198, 191, 185], [194, 187,...\n",
       "2  00087b01.jpg  w_da2efe0  [[[180, 180, 180], [178, 178, 178], [183, 183,...\n",
       "3  001296d5.jpg  w_19e5482  [[[118, 139, 218], [119, 140, 219], [120, 141,..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "\n",
    "test_proportion = 0.8\n",
    "cutoff_index = int(len(data) * test_proportion)\n",
    "\n",
    "training_data = data.iloc[:cutoff_index].reset_index(drop=True)\n",
    "test_data = data.iloc[cutoff_index:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "training_counts = Counter(training_data['Id'])\n",
    "training_data['Id_count'] = training_data.apply(lambda x: training_counts.get(x[\"Id\"]), axis=1)\n",
    "\n",
    "test_counts = Counter(test_data['Id'])\n",
    "test_data['Id_count'] = test_data.apply(lambda x: test_counts.get(x[\"Id\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_triple(data):\n",
    "#     filtered = data[data['Id_count'] > 1].reset_index(drop=True)\n",
    "    filtered = data[(data.Id_count > 1) & (data.Id != 'new_whale')].reset_index(drop=True)\n",
    "    \n",
    "    anchor_index = randint(0,len(filtered)-1)\n",
    "    anchor_image = filtered['image_array'][anchor_index]\n",
    "    anchor_id = filtered['Id'][anchor_index]   \n",
    "    same_id = anchor_id\n",
    "    relevant_indices = list(filtered.index[filtered['Id'] == anchor_id])\n",
    "    same_index = np.random.choice(relevant_indices)\n",
    "    same_image = filtered['image_array'][same_index]\n",
    "    \n",
    "    different_id = anchor_id\n",
    "    while (anchor_id == different_id):\n",
    "        different_index = randint(0,len(data)-1)\n",
    "        different_id = data['Id'][different_index]\n",
    "    \n",
    "    different_image = data['image_array'][different_index]\n",
    "#     anchor_image = random_transform(anchor_image)\n",
    "#     same_image = random_transform(same_image)\n",
    "#     different_image = random_transform(different_image)\n",
    "    return anchor_image, same_image, different_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triple_generator(batch_size, data, resize_shape):\n",
    "    while True:\n",
    "        anchor_batch = np.zeros((batch_size, resize_shape[0],resize_shape[1],resize_shape[2]))\n",
    "        same_image_batch = np.zeros((batch_size, resize_shape[0],resize_shape[1],resize_shape[2]))\n",
    "        different_image_batch = np.zeros((batch_size, resize_shape[0],resize_shape[1],resize_shape[2]))\n",
    "        for i in range(batch_size):\n",
    "                anchor_batch[i,:,:,:], same_image_batch[i,:,:,:], different_image_batch[i,:,:,:] = get_triple(data)\n",
    "\n",
    "        batches = [anchor_batch, same_image_batch, different_image_batch]\n",
    "        yield batches, np.ones(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def L2_distance(X):\n",
    "\n",
    "#     encoded_l, encoded_r = X\n",
    "\n",
    "#     # BPR loss\n",
    "#     loss = 1.0 - K.sigmoid(\n",
    "#         K.sum(user_latent * positive_item_latent, axis=-1, keepdims=True) -\n",
    "#         K.sum(user_latent * negative_item_latent, axis=-1, keepdims=True))\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = resize_shape\n",
    "\n",
    "anchor_input = Input(input_shape)\n",
    "same_category_input = Input(input_shape)\n",
    "different_category_input = Input(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n",
      "/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:78: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"lo...)`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def bpr_triplet_loss(X):    \n",
    "    alpha = 0.1\n",
    "\n",
    "    anchor_embedding, same_embedding, different_embedding = X\n",
    "    \n",
    "    positive_distance = K.square(anchor_embedding - same_embedding)\n",
    "    negative_distance = K.square(anchor_embedding - different_embedding)\n",
    "    \n",
    "    positive_distance = K.mean(positive_distance, axis=-1, keepdims=True)\n",
    "    negative_distance = K.mean(negative_distance, axis=-1, keepdims=True)\n",
    "    \n",
    "    loss = K.maximum(0.0, alpha + positive_distance - negative_distance)\n",
    "   \n",
    "    return K.mean(loss)\n",
    "\n",
    "#Something like this:\n",
    "def triplet_loss(inputs, dist='sqeuclidean', margin='maxplus', alpha=0.1):\n",
    "    anchor, positive, negative = inputs\n",
    "    positive_distance = K.square(anchor - positive)\n",
    "    negative_distance = K.square(anchor - negative)\n",
    "    if dist == 'euclidean':\n",
    "        positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True))\n",
    "        negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n",
    "    elif dist == 'sqeuclidean':\n",
    "        positive_distance = K.mean(positive_distance, axis=-1, keepdims=True)\n",
    "        negative_distance = K.mean(negative_distance, axis=-1, keepdims=True)\n",
    "    loss = positive_distance - negative_distance\n",
    "    if margin == 'maxplus':\n",
    "        loss = K.maximum(0.0, alpha + loss)\n",
    "    elif margin == 'softplus':\n",
    "        loss = K.log(alpha + K.exp(loss))\n",
    "    return K.mean(loss)\n",
    "\n",
    "\n",
    "# transfer_learning_net = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = resize_shape)\n",
    "transfer_learning_net = applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=resize_shape)\n",
    "\n",
    "for layer in transfer_learning_net.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = transfer_learning_net.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dense(512, activation=\"linear\")(x)\n",
    "predictions = Lambda(lambda x: K.l2_normalize(x,axis=1))(x)\n",
    "embedding = Model(input = transfer_learning_net.input, output = predictions)  \n",
    "    \n",
    "    \n",
    "# convnet = Sequential()\n",
    "# convnet.add(Conv2D(filters=8, input_shape=(\n",
    "#     resize_shape[0],resize_shape[1],resize_shape[2],), kernel_size=5, activation='relu',name='conv_1'))\n",
    "# convnet.add(MaxPooling2D(pool_size=2,name='pool_1'))\n",
    "# convnet.add(Conv2D(filters=12, kernel_size=3, activation='relu',name='conv_2'))\n",
    "# convnet.add(MaxPooling2D(pool_size=2,name='pool_2'))\n",
    "# convnet.add(Conv2D(filters=16, kernel_size=3, activation='relu',name='conv_3'))\n",
    "# convnet.add(MaxPooling2D(pool_size=2,name='pool_3'))\n",
    "# convnet.add(Conv2D(filters=20, kernel_size=3, activation='relu',name='conv_4'))\n",
    "# convnet.add(Conv2D(filters=32, kernel_size=3, activation='relu',name='conv_5'))\n",
    "# convnet.add(MaxPooling2D(pool_size=2,name='pool_4'))\n",
    "# convnet.add(Flatten())\n",
    "# convnet.add(Dense(units=128, activation='relu',name='dense_1'))\n",
    "# convnet.add(Dense(units=128, activation='relu',name='dense_2'))\n",
    "# convnet.add(Dense(units=64, activation='linear',name='dense_3'))\n",
    "# convnet.add(Lambda(lambda x: K.l2_normalize(x,axis=1), name='normalize'))\n",
    "\n",
    "anchor = embedding(anchor_input)\n",
    "same = embedding(same_category_input)\n",
    "different = embedding(different_category_input)\n",
    "\n",
    "loss = merge(\n",
    "        [anchor, same, different],\n",
    "        mode=triplet_loss,\n",
    "        name='loss',\n",
    "        output_shape=(1, ))\n",
    "    \n",
    "siamese_net = Model(input=[anchor_input,same_category_input, different_category_input],output=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 10\n",
    "siamese_net.compile(loss=identity_loss,optimizer=SGD(LEARNING_RATE))\n",
    "# siamese_net.compile(loss=identity_loss,optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "training_data_generator = triple_generator(BATCH_SIZE, training_data, resize_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 32s - loss: 0.0986    \n",
      "Epoch 2/100\n",
      " 71/100 [====================>.........] - ETA: 8s - loss: 0.0981"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-158:\n",
      "Process Process-159:\n",
      "Process Process-145:\n",
      "Process Process-157:\n",
      "Process Process-160:\n",
      "Process Process-150:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 569, in data_generator_task\n",
      "    self.queue.put(generator_output)\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 569, in data_generator_task\n",
      "    self.queue.put(generator_output)\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 569, in data_generator_task\n",
      "    self.queue.put(generator_output)\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 569, in data_generator_task\n",
      "    self.queue.put(generator_output)\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 569, in data_generator_task\n",
      "    self.queue.put(generator_output)\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 569, in data_generator_task\n",
      "    self.queue.put(generator_output)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/edward_bartrum/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-151c7c3cc1c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                     use_multiprocessing=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = siamese_net.fit_generator(training_data_generator,\n",
    "                                    verbose=1, \n",
    "                                    epochs=100, \n",
    "                                    steps_per_epoch=100,\n",
    "                                    workers=16,\n",
    "                                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title(\"Loss function\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data_generator = triple_generator(BATCH_SIZE, test_data, resize_shape)\n",
    "evaluation_steps = 20\n",
    "metric_names = siamese_net.metrics_names\n",
    "metric_values = siamese_net.evaluate_generator(evaluation_data_generator, steps=evaluation_steps)\n",
    "print(\"Metric names\", metric_names)\n",
    "print(\"Metric values\", metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "now = time.strftime('%Y.%m.%d %H:%M:%S')\n",
    "directory = \"weights/\" + now + \"/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "\n",
    "siamese_net.save_weights(directory + \"siamese_weights\")\n",
    "convnet.save_weights(directory + \"convnet_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "file_list = data['Image']\n",
    "image_list = [get_image(f) for f in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_list = data['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = convnet.predict(np.stack(image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_image(file, shape=(resize_shape[0],resize_shape[1])):\n",
    "    image = Image.open('data/test/' + file)\n",
    "    image = image.resize(shape)\n",
    "    image = np.array(image)\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.stack([image]*3,axis=2) \n",
    "    return image\n",
    "\n",
    "\n",
    "sample_sub = pd.read_csv(\"data/sample_submission.csv\")\n",
    "submission_file_list = sample_sub['Image']\n",
    "submission_image_list = [get_sub_image(f) for f in submission_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_embedding_list = convnet.predict(np.stack(submission_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1,2,3,4]\n",
    "Y = [9,8,2,3]\n",
    "\n",
    "[x for (y,x) in sorted(zip(Y,X), key=lambda pair: pair[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(li):\n",
    "    my_set = set()\n",
    "    filtered = []\n",
    "    for e in li:\n",
    "        if e not in my_set:\n",
    "            filtered.append(e)\n",
    "            my_set.add(e)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def classify(image_embedding, embedding_list, id_list, num_categories=5):\n",
    "    image_embedding = np.expand_dims(image_embedding, axis=0)\n",
    "    stacked_image = np.repeat(image_embedding,len(embedding_list),axis=0)\n",
    "    square_differences = (stacked_image - embedding_list)**2\n",
    "    scores = np.sum(square_differences, axis=1)    \n",
    "    sorted_ids = [x for (y,x) in sorted(\n",
    "        zip(scores,id_list), key=lambda pair: pair[0])]\n",
    "    \n",
    "    return ' '.join(remove_duplicates(sorted_ids[0:num_categories]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(embedding_list[0], embedding_list, id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_embedding_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_prediction_list = [classify(image_embedding, embedding_list, id_list) for image_embedding in submission_embedding_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'Image': submission_file_list, 'Id': submission_prediction_list}, columns=['Image','Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"results/submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
